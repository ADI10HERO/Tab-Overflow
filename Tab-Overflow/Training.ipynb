{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "3EyAZ_sRUK10",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cyvBduZEUTMv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dataset = pd.read_csv('dataset.tsv', delimiter = '\\t', quoting = 3,error_bad_lines=False)\n",
        "print(dataset)\n",
        "\n",
        "corpus = []\n",
        "for i in range(0, len(dataset)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', dataset['Title'][i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    ps = PorterStemmer()\n",
        "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)\n",
        "\n",
        "cv = CountVectorizer(max_features=15000)\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "y = dataset.iloc[:, 3].values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P0uPWQFYUUuq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.40)\n",
        "#print(X_train,X_test,y_train,y_test)\n",
        "\n",
        "y_train2=list(set(y_train))\n",
        "allwords={}\n",
        "for i in range(len(y_train2)):\n",
        "  allwords[i]=y_train2[i]\n",
        "  allwords[y_train2[i]]=i\n",
        "  \n",
        "#print(allwords)\n",
        "\n",
        "X_train1=[[int(X_train[i][j]) for j in range(len(X_train[i]))] for i in range(len(X_train))]\n",
        "y_train1=[allwords[i] for i in list(y_train)]\n",
        "for i in range(len(y_train1)):\n",
        "  index=y_train1[i];\n",
        "  y_train1[i]=[0 for j in range(13)]\n",
        "  y_train1[i][index]=1\n",
        "  \n",
        "X_test1=[[int(X_test[i][j]) for j in range(len(X_test[i]))] for i in range(len(X_test))]\n",
        "y_test1=[allwords[i] for i in list(y_test)]\n",
        "for i in range(len(y_test1)):\n",
        "  index=y_test1[i];\n",
        "  y_test1[i]=[0 for j in range(13)]\n",
        "  y_test1[i][index]=1\n",
        "\n",
        "print(y_train1)\n",
        "\n",
        "data={\"X_train\":X_train1,\"X_test\":X_test1,\"y_train\":y_train1,\"y_test\":y_test1,\"allwords\":allwords}\n",
        "print(data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GbLLpeYoUYB7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(\"data1.json\",\"w\") as filename:\n",
        "    json.dump(data,filename)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D3Ru4cIwUZIL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "from random import random\n",
        "\n",
        "no_of_inputs=len(data[\"X_train\"][0])\n",
        "no_of_output=len(data[\"y_train\"][0])\n",
        "\n",
        "\n",
        "#no_of_inputs=547 and o_of_outputs=13 for the dataset\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Dense(50,input_shape=(no_of_inputs,),activation=\"relu\"))\n",
        "model.add(Dense(20,activation=\"relu\"))\n",
        "model.add(Dense(no_of_output,activation=\"softmax\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Kvald1zUZ5u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(np.array(data[\"X_train\"]), np.array(data[\"y_train\"]), epochs=50, batch_size=10)\n",
        "\n",
        "#Getting the weights of the trained neural network\n",
        "weights=model.get_weights()\n",
        "\n",
        "prediction=model.predict(np.array(data[\"X_test\"]))\n",
        "count=0\n",
        "for i in range(len(prediction)):\n",
        "  #print(prediction[i])\n",
        "  #print(np.argmax(prediction[i]),np.argmax(data[\"y_test\"][i]))\n",
        "  if np.argmax(prediction[i])==np.argmax(data[\"y_test\"][i]):\n",
        "    count+=1\n",
        "print(\"Accuracy is:-\",(count/len(prediction))*100)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jaj2d3PJUdvF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(weights)\n",
        "print(allwords)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}